{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: node(s) that can represent the input or output of an neural net\n",
    "### Hidden Layer: node(s) who do not have a discreet output or input. they handle weighting\n",
    "### Output Layer: output node(s) of neural net and can be used to train or feed back into the input layer\n",
    "### Neuron: represents a node in the net, and will take input and make output.\n",
    "### Weight: input that creates bias in a net\n",
    "### Activation Function: activates the output, given the result of a net of bias\n",
    "### Node Map: generlized map of flow from input to output in a neural net design\n",
    "### Perceptron: single neuron nuerual net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "## Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "## My Answer Here\n",
    "### input -> hidden layer x weight -> = bias x -> output -(>activation function)  \n",
    "### this flow can be encapsulated as a single neuron or a network of specially crated hidden layers, but the principle remains the same input flows to/through layers and creates a bias, that bias is fed into the next layer, or the output if there are no other layers, and is captured by the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "np.random.seed(42)\n",
    "\n",
    "data = np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "\n",
    "\n",
    "inputs = np.array(([1], [1], [1], [0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic function that enables learning \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# uses derivative of sigmoid to find loss in a step\n",
    "def loss_function(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# activates ouput by taking product of inputs and weights\n",
    "def activation(x, weight):\n",
    "    return sigmoid(np.dot(x, weight))\n",
    "\n",
    "# lol more abstract becasue it sless clear, but takes a \n",
    "# error and looks at loss. thats it thats my understnading.\n",
    "def backpropagation(activation, y):\n",
    "    error = y - activation\n",
    "    return error * loss_function(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19731697],\n",
       "       [-0.68796272],\n",
       "       [-0.68801096]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-8.57694879]\n",
      " [-8.57694882]\n",
      " [12.91425466]]\n",
      "Output after train\n",
      "[[0.99999753]\n",
      " [0.9870905 ]\n",
      " [0.9870905 ]\n",
      " [0.01421508]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "    \n",
    "    # Activate!\n",
    "    output = activation(data, weights)\n",
    "    backprop = backpropagation(output, inputs)\n",
    "    weights += np.dot(data.T, backprop)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after train\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35294118, 0.74371859, 0.59016393, ..., 0.23441503, 0.48333333,\n",
       "        1.        ],\n",
       "       [0.05882353, 0.42713568, 0.54098361, ..., 0.11656704, 0.16666667,\n",
       "        1.        ],\n",
       "       [0.47058824, 0.91959799, 0.52459016, ..., 0.25362938, 0.18333333,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.29411765, 0.6080402 , 0.59016393, ..., 0.07130658, 0.15      ,\n",
       "        1.        ],\n",
       "       [0.05882353, 0.63316583, 0.49180328, ..., 0.11571307, 0.43333333,\n",
       "        1.        ],\n",
       "       [0.05882353, 0.46733668, 0.57377049, ..., 0.10119556, 0.03333333,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = MinMaxScaler().fit_transform(diabetes[feats])\n",
    "X = np.hstack((X, np.ones((diabetes.shape[0],1))))\n",
    "y = diabetes[['Outcome']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10, rate=0.01):\n",
    "        self.niter = niter\n",
    "        self.rate = rate\n",
    "        \n",
    "    def __sigmoid(self, x):\n",
    "        self.x = x\n",
    "        return 1 / (1 + np.exp(-self.x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        self.x = sigmoid(x)\n",
    "        return self.x * (1 - self.x)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        self.weights = np.zeros((X.shape[1], 1))\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            output = self.__sigmoid(np.dot(X, self.weights))\n",
    "            error = self.rate * (y - output)\n",
    "            adjustments = error * self.__sigmoid_derivative(output)\n",
    "            self.weights += np.dot(X.T, adjustments)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(niter=1000)\n",
    "p.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = p.predict(X)\n",
    "pred = (p.predict(X) + 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799479166666666"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = (p.predict(X) + 0.5).astype(int)\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334,\n",
       " 0.7708333333333334]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(300):\n",
    "    p = Perceptron(niter=i)\n",
    "    p.fit(X, y)\n",
    "    pred = (p.predict(X) + 0.5).astype(int)\n",
    "    scores.append(accuracy_score(pred, y))\n",
    "scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.rcParams['figure.facecolor'] = '#002B36'\n",
    "pyplot.rcParams['axes.facecolor'] = 'black'\n",
    "pyplot.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "pyplot.plot(scores)\n",
    "pyplot.yticks([i/100 for i in range(35,76,5)])\n",
    "pyplot.ylabel('Accuracy')\n",
    "pyplot.xlabel('Epochs')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
